{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5277141-1356-4c1d-9bb6-a367d4230ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 16:19:32.351163: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/var/folders/gt/_59ppl5169347x60jbn7ky1c0000gq/T/ipykernel_67571/chat.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gt/_59ppl5169347x60jbn7ky1c0000gq/T/ipykernel_67571/2248398417.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# importiere das Dialog-design\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetJsonPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mdialogflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/var/folders/gt/_59ppl5169347x60jbn7ky1c0000gq/T/ipykernel_67571/chat.json'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "stemmer = GermanStemmer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import random\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "def getPath(file):\n",
    "    path = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "    path = os.path.join(path, file).replace(\"\\\\\", \"/\")\n",
    "    return path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def getJsonPath():\n",
    "    path = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "    path = os.path.join(path, 'chat.json').replace(\"\\\\\", \"/\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# importiere das Dialog-design\n",
    "with open(getJsonPath(), encoding='UTF-8') as json_data:\n",
    "    dialogflow = json.load(json_data)\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "stop= stopwords.words('german')\n",
    "ignore_words = ['?', '.', ','] + stop\n",
    "# loop durch jeden Satz in unseren dialogflow und synonym\n",
    "for dialog in dialogflow['dialogflow']:\n",
    "    for pattern in dialog['synonym']:\n",
    "        # Tokenisieren jedes Wort im Satz\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # füge die zu unserer Wörterliste hinzu\n",
    "        words.extend(w)\n",
    "        # füge die zu Dokumenten in unserem Korpus hinzu\n",
    "        documents.append((w, dialog['intent']))\n",
    "        # füge die zu unserer Klassenliste hinzu\n",
    "        if dialog['intent'] not in classes:\n",
    "            classes.append(dialog['intent'])\n",
    "\n",
    "# stemme jedes Word und entferne Duplikate\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words] + ['weit', 'and', 'nicht']\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# sortiere unsere Klassen\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"Docs\")\n",
    "print(len(classes), \"Classes\", classes)\n",
    "print(len(words), \"Split words\", words)\n",
    "\n",
    "# erstelle unsere training data\n",
    "training = []\n",
    "output = []\n",
    "# Erstelle ein leeres Array für unsere Output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# generiere training set und bag of words für jeden Satz\n",
    "for doc in documents:\n",
    "    # Initialisierung unsere bag of words\n",
    "    bag = []\n",
    "    # Liste der tokenisierte Wörter für den synonym\n",
    "    pattern_words = doc[0]\n",
    "    # stemme jedes Wort\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    #  erstelle unsre bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    # output ist '0' für jedes intent und '1' für das aktuelle intent\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# mische unsere Features und verwandle die in np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# Erstelle die Training-Liste\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "#tf.compat.v1.reset_default_graph()\n",
    "# Aufbau des neuronalen Netzes\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 88)\n",
    "net = tflearn.fully_connected(net, 88)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Definiere das Modell und konfiguriere tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir=getPath('train_logs'))\n",
    "# Starte das training des Modells\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=256, show_metric=True)\n",
    "# Speichere das trainirte Modell\n",
    "model.save(getPath('model.tflearn'))\n",
    "\n",
    "print(\"model created\")\n",
    "#Bearbeitung der Benutzereingaben, um einen bag-of-words zu erzeugen\n",
    "def frageBearbeitung(frage):\n",
    "    # tokenisiere die synonymen\n",
    "    sentence_word = nltk.word_tokenize(frage, language='german')\n",
    "    # generiere die Stopwörter\n",
    "    stop= stopwords.words('german')\n",
    "    ignore_words = ['?', '.', ','] + stop\n",
    "    ######Korrektur Schreibfehler\n",
    "    sentence_words=[]\n",
    "    for word in sentence_word:\n",
    "        if word not in ignore_words or word=='weiter' or word=='andere' or word=='nicht':\n",
    "            #a=correction(word)\n",
    "            sentence_words.append(word)\n",
    "    # stemme jedes Wort\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# Rückgabe bag of words array: 0 oder 1 für jedes Wort in der 'bag', die im Satz existiert\n",
    "def bow(frage, words, show_details=False):\n",
    "    sentence_words = frageBearbeitung(frage)\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"found in bag: %s\" % w)\n",
    "\n",
    "    return (np.array(bag))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ERROR_THRESHOLD=0\n",
    "def klassifizieren(frage):\n",
    "    # generiere Wahrscheinlichkeiten von dem Modell\n",
    "\n",
    "    p = bow(frage, words, show_details=False)\n",
    "    results = model.predict(np.array([p]))[0]\n",
    "\n",
    "        # herausfiltern Vorhersagen unterhalb eines Schwellenwerts\n",
    "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "        # nach Stärke der Wahrscheinlichkeit sortieren\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    return return_list\n",
    "print(klassifizieren('hallo'))\n",
    "\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Mach mir eine Frage!!(Type 'quit' to exit)\")\n",
    "    while True:\n",
    "        inp = input(\"\\nYou: \")\n",
    "        if inp.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "    #Porbability of correct response\n",
    "        results = klassifizieren(inp)\n",
    "        for tg in dialogflow['dialogflow']:\n",
    "\n",
    "            if tg['intent'] == results[0][0]:\n",
    "                antwort = tg['antwort']\n",
    "                print(\"Bot:\" + random.choice(antwort))\n",
    "\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e58069-9571-4768-b262-5b1ca8ce94da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
